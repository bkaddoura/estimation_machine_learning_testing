---
title: "class_6"
output: html_document
date: "2023-03-04"
---

Let's attach our libraries. We are using a couple of new ones. Install them with `install.packages('')` if you do not have them.
```{r, message=FALSE}
#libraries
library(ISLR2)
library(ggplot2)
library(splines)
library(gam)
```
We'll use the `Wage` dataset (new to us). As always, take a look at it (using `summary`, `names`, `str`, `dim`, etc., to understand its contents).
```{r}
attach(Wage)
names(Wage)
str(Wage)
```
# Polynomial regression

Let's fit a polynomial regression. We can use the `poly` function. In this case, let's fit with "the fourth order polynomial".
```{r}
#fit our model
poly.fit <- lm(wage ~ poly(age, 4), data=Wage) #4 means 4th order

#alternative way to fit (identical)
poly.fit2 <- lm(wage ~ cbind(age, age^2, age^3, age^4), data=Wage)

#as always, we can look at a summary of the model 
summary(poly.fit)

#in particular, let's look at the coefficients, using the `coef` "convenience function"
coef(summary(poly.fit))

#we need a vector saying for which values to make predictions
#we want to make predictions for every age value between the minimum and maximum age value in the dataset
age.range <- seq(from=min(age), to=max(age)) #will be values 18 through 80
age.range2 <- min(age):max(age) #an alternative way to make predictions (identical)

#now, let's make predictions
pred.poly <- predict(poly.fit, newdata=list(age=age.range), se=TRUE) #we want SE

#we can create confidence intervals around our predictions
conf.int <- cbind(pred.poly$fit + 2 * pred.poly$se.fit, pred.poly$fit- 2 * pred.poly$se.fit) #2 is roughly equal to a 95% confidence interval: if you want to be more precise, use 1.96 (exactly a 95% confidence interval)

#now that we all our component parts, let's pull them together in a single dataframe
#technically, this is optional -- but it makes plotting much easier (and is better to stay organized)
predictions.poly <- data.frame(AGE = age.range, WAGE = pred.poly$fit, upper=conf.int[, 1], lower=conf.int[, 2])

#finally, we can make our plot 
#note, this is saving our plot to an object plot poly -- to show it below, simply do not run the first line, i.e., plot.poly.
plot.poly <- 
  ggplot() + 
  ggtitle('Polynomial (3rd degree)') +
  theme_classic() + #makes things look pretty
  geom_ribbon(data = predictions.poly, aes(x=age.range, ymin=lower, ymax=upper), alpha=.2) + #our confidence intervals
  geom_point(data = Wage, aes(x=age, y=wage), size=1, alpha=.5) + #the points 
  geom_line(data = predictions.poly, aes(x=AGE, y=WAGE), col='blue') + #the line
  xlab('mens\' ages') + #the backslash ignores the apostrophe
  ylab('mens\' wages')

```

In the code above, we apriori decided that we wanted to fit a 4th order polynomial. However, we can statistically determine which order of polynomial fits the data best!

```{r}

#fit separate models with different degrees
fit1 <- lm(wage ~ age, data=Wage) #this is linear
fit2 <- lm(wage ~ poly(age, 2), data=Wage) #quad
fit3 <- lm(wage ~ poly(age, 3), data=Wage) #cubic
fit4 <- lm(wage ~ poly(age, 4), data=Wage) #quartic
fit5 <- lm(wage ~ poly(age, 5), data=Wage) #quintic -- by convention, we stop at quartic

#now, we can perform anova to compare models
anova(fit1, fit2, fit3, fit4, fit5) #must be in order of complexity (most simple first)

```
The top section of this output reminds us of the 5 models we fit. We make sure we did what we inteded, but otherwise ignore this output.

For model results, we care about the bottom section of this output. We interpret the p values `(Pr(>F))` column pairwise (note that `(Pr(>F))` stands for probability greater than F, i.e., a p value). Our pairwise interpretation means that we interpret the first two rows together, and then the second and third row together, and the third and fourth row together, and so forth.

The first two rows compare the value of a one degree polynomial (i.e., the linear model) and the second order (quadratic). The three asterisks tell us that the model is significant, at a 
high level (p<.001). (See the `Signif. codes` legend along the bottom). So, this leads us to say that the second order polynomial is better than the linear model.

But we are not done! We go on to interpret the second and third row in conjunction. Here, we see two asterisks in the third row. This means that the third order (more complex) model is better than the second. This is so at a p value level of p <.01. 

Likewise, we proceed to compare the third and fourth rows. Here, the fourth row has a `.` (period). When we check the `Signif. codes` legend, we see this is not a significant difference, though it is "trending" or "approaching" significance, at p<.1. We conservatively say that the fourth order polynomial is not preferable to the third order.

We conclude here that the third order polynomial offers the best fit, in this data.

# Step Function

```{r}

#let's fit our step model
step.fit <- lm(wage ~ cut(age, 4), data=Wage) #here, the 4 indicates NUMBER OF CUTS, not the polynomial order! This is simply a linear regression with cuts.

#let's look at the coefficients
coef(summary(step.fit))

#we can visualize our model, as before
#now, let's make predictions
pred.step <- predict(step.fit, newdata=list(age=age.range), se=TRUE) #we want SE

#we can create confidence intervals around our predictions
conf.int <- cbind(pred.step$fit + 2 * pred.step$se.fit, pred.step$fit- 2 * pred.step$se.fit) #2 is roughly equal to a 95% confidence interval: if you want to be more precise, use 1.96 (exactly a 95% confidence interval)

#let's pull them together in a single dataframe
predictions.step <- data.frame(AGE = age.range, WAGE = pred.step$fit, upper=conf.int[, 1], lower=conf.int[, 2])

#finally, we can make our plot
plot.step <- 
  ggplot() + 
  ggtitle('Step function (4 steps/cuts)') +
  theme_classic() + #makes things look pretty
  geom_ribbon(data = predictions.step, aes(x=age.range, ymin=lower, ymax=upper), alpha=.2) + #our confidence intervals
  geom_point(data = Wage, aes(x=age, y=wage), size=1, alpha=.5) + #the points 
  geom_line(data = predictions.step, aes(x=AGE, y=WAGE), col='blue') + #the line
  xlab('mens\' ages') + #the backslash ignores the apostrophe
  ylab('mens\' wages')

```
Interpretation of coefficients here is a bit complicated. The `Estimate` associated with the `Intercept` is the mean value at the first cut. The subsequent rows should the relative average increase/decrease to this first group.

We can verify this as follows. 
```{r}

#first, let's set up levels (many ways to do this -- this may be easiest)
#let's find out where to make cuts in age
number_of_cuts <- 4
age_range <- max(Wage$age) - min(Wage$age) #62
age_band <- age_range / number_of_cuts
cut1_max <- min(Wage$age) + age_band
cut2_max <- min(Wage$age) + (age_band * 2)
cut3_max <- min(Wage$age) + (age_band * 3)

#calculate means
cut1_mean <- mean(Wage$wage[Wage$age < cut1_max])
cut2_mean <- mean(Wage$wage[Wage$age > cut1_max & Wage$age <= cut2_max])
cut3_mean <- mean(Wage$wage[Wage$age > cut2_max & Wage$age < cut3_max])
cut4_mean <- mean(Wage$wage[Wage$age > cut3_max])

#let's pull out the coefficient means from the model
coefficients <- coef(summary(step.fit))
cut1_mean_coef <- coefficients[1, 1]
cut2_mean_coef <- coefficients[2, 1]
cut3_mean_coef <- coefficients[3, 1]
cut4_mean_coef <- coefficients[4, 1]

#are these the same? check rounding
round(cut1_mean, 4) == round(cut1_mean_coef, 4) #yes, 94.158392

```

# Splines

```{r}
#fit a new spline model -- we'll specify the 'knots' this time
spline.fit <- lm(wage ~ bs(age, knots=c(30, 40, 50)), data=Wage) #bs = base function spline
                 
#make predictions
pred.spline <- predict(spline.fit, newdata=list(age=age.range), se=TRUE)

#confidence intervals
conf.int <- cbind(pred.spline$fit + 1.96 * pred.spline$se.fit, pred.spline$fit - 1.96 * pred.spline$se.fit) #1.96 a bit more precise than 2 -- equivalent to 95% CI

#make a nice dataframe to work with
predictions.spline <- data.frame( #note, we can break up code for easier reading
   AGE = age.range, 
   WAGE=pred.spline$fit, 
   upper=conf.int[,1],
   lower=conf.int[,2])

#plot
plot.spline <- 
ggplot() + 
  ggtitle('Spline (knots=30,40,50)') + 
  geom_ribbon(data=predictions.spline, aes(x=age.range, ymin=lower, ymax=upper), alpha=.4) +
  geom_point(data=Wage, aes(x=age, y=wage), alpha=.2) +
  geom_line(data=predictions.spline, aes(x=AGE, y=WAGE), col='red') +
  xlab('mens\' ages') + 
  ylab('mens\' wages')
```
Compare to what happens when we 'knot' where we don't have much (or any) age data, e.g., change your knots to (15, 50, 90).

# Local Regression

```{r}
#fit our model -- notice using loess function
loess.fit <- loess(wage ~ age, span=.5, data=Wage)

#prediction
pred.loess <- predict(loess.fit, data.frame(age=age.range), se=TRUE) #note! the loess works with data best in a dataframe, not in a list

#confidence intervals
conf.int <- cbind(pred.loess$fit + 1.96 * pred.loess$se.fit, pred.loess$fit - 1.96 * pred.loess$se.fit)

#make a nice dataframe to work with
predictions.loess <- data.frame(
  AGE = age.range, 
  WAGE=pred.loess$fit, 
  upper=conf.int[,1],
  lower=conf.int[,2])

#plot
plot.loess <- 
  ggplot() + 
  ggtitle('Local Regression (LOESS)') +
  geom_ribbon(data=predictions.loess, aes(x=age.range, ymin=lower, ymax=upper), alpha=.4) +
  geom_point(data=Wage, aes(x=age, y=wage), alpha=.2) +
  geom_line(data=predictions.loess, aes(x=AGE, y=WAGE), col='red')+
  xlab('mens\' ages') + 
  ylab('mens\' wages')
  
```

It's usually good practice to compare our visualizations side by side. We can do that, if we save our plots to an object (just like we would a variable, or a dataframe!). Then, we can using a package related to ggplot, to render them as we like.

```{r}
library(gridExtra)
grid.arrange(plot.poly, plot.step, plot.spline, plot.loess, ncol=2)
```


You might also be thinking: that is a _lot_ of repeated code for plotting. You'd be right. We can cut down on repeated/redundant code (which opens up opportunity for errors or "bugs") by writing a general plotting function, and passing in unique plotting requirements. Here's how to do that:

```{r}
#general plotting function for these curves
#we have only two unique bits of info: our predictions dataframe, and the title
plot_fn <- function(my_predictions, my_title){
  ggplot () +
  ggtitle(my_title) +
  geom_ribbon(data=my_predictions, aes(x=age.range, ymin=lower, ymax=upper), alpha=.4) +
  geom_point(data=Wage, aes(x=age, y=wage), alpha=.2) +
  geom_line(data=my_predictions, aes(x=AGE, y=WAGE), col='red')+
  xlab('mens\' ages') + 
  ylab('mens\' wages')
}

#let's make our plots (remember to provide the arguments!)
plot1<- plot_fn(my_predictions=predictions.poly, my_title='Polynomial')
plot2<- plot_fn(predictions.step, 'Step')
plot3<- plot_fn(predictions.spline, 'Spline')
plot4<- plot_fn(predictions.loess, 'Local')

#plot all together using gridExtra
grid.arrange(plot1, plot2, plot3, plot4, ncol=2)

```




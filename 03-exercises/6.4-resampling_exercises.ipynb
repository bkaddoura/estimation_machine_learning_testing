{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ee216ac-cbc1-4576-a6c2-74e6ae2ff2c9",
   "metadata": {},
   "source": [
    "# 6.4: Resampling Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451fa372-7eb2-43e8-bbf5-429bc446f0a0",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "### Import Libraries \n",
    "\n",
    "We import our standard libraries and specific objects/libraries at the top level of our notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6edbc31d-be7c-4dd3-bb07-a9f681754433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our previous libraries and objects\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from ISLP import load_data\n",
    "from ISLP.models import (ModelSpec as MS,\n",
    "                         summarize,\n",
    "                         poly)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import new libraries and objects\n",
    "from functools import partial\n",
    "from sklearn.model_selection import \\\n",
    "     (cross_validate,\n",
    "      KFold,\n",
    "      ShuffleSplit)\n",
    "from sklearn.base import clone\n",
    "from ISLP.models import sklearn_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f01389-fd9e-4824-8601-44ce42f58f98",
   "metadata": {},
   "source": [
    "## The Validation Set Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e417696-f8c8-4ef5-8404-e6838a907d47",
   "metadata": {},
   "source": [
    "We explore the use of the validation set approach in order to estimate the test error rates that result from fitting various linear models on the `Auto` data set.\n",
    "\n",
    "We use the function `train_test_split()` to split the data into training and validation sets. As there are 392 observations, we split into two equal sets of size 196 using the argument `test_size=196`. We want to set a random seed (`random_state=0`) so that the random results can be reproduced again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0419abe2-c733-4124-b202-45f6a8bf953e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Auto = load_data('Auto')\n",
    "Auto_train, Auto_valid = train_test_split(Auto,\n",
    "                                         test_size=196, # split in two\n",
    "                                         random_state=0) # random seed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6667129-56af-4d42-9416-2b51e60e3f0c",
   "metadata": {},
   "source": [
    "Now we can fit a linear regression using only the observations corresponding to the training set `Auto_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c531b5b-803b-42ed-88f0-e20242e0ad85",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_mm = MS(['horsepower'])\n",
    "X_train = hp_mm.fit_transform(Auto_train)\n",
    "y_train = Auto_train['mpg']\n",
    "model = sm.OLS(y_train, X_train)\n",
    "results = model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af93fbe6-df41-4f9f-878a-8b58b38fd0a0",
   "metadata": {},
   "source": [
    "We now use the `predict()` method of results evaluated on the model matrix for this model created using the validation data set. We also calculate the validation MSE of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dda5e219-dcd0-4569-a6b3-02954d68d2fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.61661706966988"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid = hp_mm.transform(Auto_valid)\n",
    "y_valid = Auto_valid['mpg']\n",
    "valid_pred = results.predict(X_valid)\n",
    "np.mean((y_valid - valid_pred)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252d8d71-405e-4b5f-92ea-365e44b2a8c1",
   "metadata": {},
   "source": [
    "So our estimated test MSE for the linear regression is 23.62.\n",
    "\n",
    "**What would happen if we used a different set of observations for our training set? Repeat the above\n",
    "process using a different seed (i.e. set.seed(2)) and report the differences.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c467449e-0c6d-413d-8f2c-97dccfbc5e5d",
   "metadata": {},
   "source": [
    "## Leave-One-Out Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa9709e-556e-4c23-81c8-3b75dd28c9a7",
   "metadata": {},
   "source": [
    "The simplest way to cross-validate in Python is to use `sklearn`. The `ISLP` package provides a wrapper (`sklearn_sm()`), that allows us to easily use the cross-validation tools of `sklear`n with models fit by `statsmodels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22117be0-29ae-4ccb-9b93-c5c538e90528",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.231513517929216"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LOOC is a specific case of cross-validation\n",
    "# creates as many folds as the cv value\n",
    "# when k=n then k-fold becomes the same as leave-one-out\n",
    "#We only know itâ€™s LOOCV because cv = number of rows in your dataset\n",
    "hp_model = sklearn_sm(sm.OLS,\n",
    "                      MS(['horsepower']))\n",
    "X, Y = Auto.drop(columns=['mpg']), Auto['mpg']\n",
    "cv_results = cross_validate(hp_model,\n",
    "                            X,\n",
    "                            Y,\n",
    "                            cv=Auto.shape[0])\n",
    "cv_err = np.mean(cv_results['test_score'])\n",
    "cv_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca753b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 392 k-folds\n",
    "Auto.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04cabad",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5caf7f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results['fit_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d5aa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results['test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719b4305",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results['test_score'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320c7084",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45839a95-4991-43d4-bc15-a529d0121252",
   "metadata": {},
   "source": [
    "The arguments to `cross_validate()` are: an object with the appropriate `fit()`, `predict()`, and `score()` methods, an array of features `X` and a response `Y`. We also included an additional argument `cv` to `cross_validate()`; specifying an integer \n",
    "$K$ results in $K$-fold cross-validation.\n",
    "\n",
    "We can repeat this procedure for increasingly complex polynomial fits. To automate the process, we again use a for loop which iteratively fits polynomial regressions of degree 1 to 5, computes the associated cross-validation error, and stores it in the $i$th element of the vector `cv_error`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "752e078f-f2a6-40c9-9597-e45a0698b8d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24.23151352, 19.24821312, 19.33498406, 19.42443033, 19.03323073])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_error = np.zeros(5)\n",
    "H = np.array(Auto['horsepower'])\n",
    "M = sklearn_sm(sm.OLS)\n",
    "for i, d in enumerate(range(1,6)):\n",
    "    X = np.power.outer(H, np.arange(d+1))\n",
    "    M_CV = cross_validate(M,\n",
    "                          X,\n",
    "                          Y,\n",
    "                          cv=Auto.shape[0])\n",
    "    cv_error[i] = np.mean(M_CV['test_score'])\n",
    "cv_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc754b70-ba8d-43ba-afed-3596d132cedf",
   "metadata": {},
   "source": [
    "**Write a for loop that fits the Auto data using polynomials up to degree 10. Compute the LOOCV\n",
    "test error rate for each of them. Which degree polynomial fits the data the best according to\n",
    "the test error rate and the bias-variance trade-off?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2386414e-7ecb-400c-8960-0a1a177f772e",
   "metadata": {},
   "source": [
    "## k-Fold Cross-Validation\n",
    "\n",
    "Here we use `KFold()` to partition the data into $K=10$ random groups. We use random_state to set a random seed and initialize a vector `cv_error` in which we will store the CV errors corresponding to the polynomial fits of degrees one to five."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16c32ccf-4e6d-412d-a89a-7f954cb731d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24.20766449, 19.18533142, 19.27626666, 19.47848399, 19.13721595])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_error = np.zeros(5)\n",
    "cv = KFold(n_splits=10,\n",
    "           shuffle=True,\n",
    "           random_state=0) # use same splits for each degree\n",
    "for i, d in enumerate(range(1,6)):\n",
    "    X = np.power.outer(H, np.arange(d+1))\n",
    "    M_CV = cross_validate(M,\n",
    "                          X,\n",
    "                          Y,\n",
    "                          cv=cv)\n",
    "    cv_error[i] = np.mean(M_CV['test_score'])\n",
    "cv_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45191e7f-a7ad-4e2b-9f43-11ec0793179c",
   "metadata": {},
   "source": [
    "**Repeat this process using K = 10 and report the results.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2be2e8-fc23-4137-8537-ec035ffdfa0f",
   "metadata": {},
   "source": [
    "## The Bootstrap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5bbf66-b518-430a-8def-51eae4edba7c",
   "metadata": {},
   "source": [
    "The bootstrap approach can be applied in almost all situations. While there are several implementations of the bootstrap in Python, its use for estimating standard error is simple enough that we write our own function below for the case when our data is stored in a dataframe. We will use the `Portfolio` data set in the `ISLP` package to illustrate.\n",
    "\n",
    "We will create a function `alpha_func()`, which takes as input a dataframe `D` assumed\n",
    "to have columns `X` and `Y`, as well as a\n",
    "vector `idx` indicating which observations should be used to\n",
    "estimate \n",
    "$\\alpha$. The function then outputs the estimate for $\\alpha$ based on\n",
    "the selected observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07cd0632-1ae7-463e-8e0f-6504fb70f3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Portfolio = load_data('Portfolio')  # starting with dataset 'Portfolio' \n",
    "# the alpha_func method is a financial measure of return over investment of something\n",
    "# if X are our stock values, Y is something you want to benchmark against\n",
    "def alpha_func(D, idx):  # bootstrapping alpha_func paramter\n",
    "   # 'D[['X','Y']].loc[idx]': This selects rows from D where idx is True (or where idx is an index list).\n",
    "   # 'np.cov(...)': This calculates the covariance matrix of the selected rows, assuming that the data in \n",
    "   # 'X' and 'Y' columns are used as variables.\n",
    "   cov_ = np.cov(D[['X','Y']].loc[idx], rowvar=False)  \n",
    "   # The return statement calculates a statistic related to bootstrapping using elements of the covariance matrix.\n",
    "   return ((cov_[1,1] - cov_[0,1]) /\n",
    "           (cov_[0,0]+cov_[1,1]-2*cov_[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5244b3fc-be11-4566-8dcf-603e0035e796",
   "metadata": {},
   "source": [
    "The following command estimates $\\alpha$ using all 100 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6e4973de-b821-4af8-8e0f-61befe6d86a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.57583207459283"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha_func(Portfolio, range(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9872d435",
   "metadata": {},
   "source": [
    "The value returned, 0.575. We want to know if this is an accuracte value or if we could get a better estimate of this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e226aa-b7cf-49cc-9b57-2b716e163457",
   "metadata": {},
   "source": [
    "Next we randomly select 100 observations from range(100), with replacement. This is equivalent to constructing a new bootstrap data set and recomputing $\\alpha$ based on the new data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "71bfd7c4-02b0-45ad-99f4-616ace0f044c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6074452469619004"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rng = np.random.default_rng(0): This initializes a random number generator (rng) with a seed value of 0.\n",
    "rng = np.random.default_rng(0)\n",
    "# rng.choice(100, 100, replace=True): This generates an array of 100 random indices between 0 and 99 with \n",
    "# replacement. This means that the same index can be picked multiple times in the sample.\n",
    "# alpha_func(Portfolio, ...): This calls the alpha_func function with the Portfolio dataset and the array \n",
    "# of random indices as arguments. The alpha_func function will then calculate the alpha statistic using the \n",
    "# selected rows from the Portfolio dataset based on the random indices.\n",
    "alpha_func(Portfolio,\n",
    "           rng.choice(100,\n",
    "                      100,\n",
    "                      replace=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e640624",
   "metadata": {},
   "source": [
    "This time our result was 0.607. This appears to be a better estimate of alpha than previously at 0.575\n",
    "\n",
    "We've done this experiment once but what we can do is automate it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee315a5-d48e-4c57-b406-c2940c04e794",
   "metadata": {},
   "source": [
    "This process can be generalized to create a simple function `boot_SE()` for computing the bootstrap standard error for arbitrary functions that take only a data frame as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c616715-c7e3-4230-a313-58ba81bdd69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# This code performs bootstrapping to estimate the distribution of a statistic (func) calculated on a dataset D. \n",
    "\n",
    "def boot_SE(func,\n",
    "            D,\n",
    "            n=None,\n",
    "            B=1000,  # run experiment 1000 times\n",
    "            seed=0):\n",
    "    # NOTE: Suppress FutureWarning in ISLP.models.columns\n",
    "    # The warning is related to Series.__getitem__ treating keys as positions, which is deprecated.\n",
    "    # Since ISLP is an external library that I don't control, and this specific warning does not\n",
    "    # affect my current usage, I'm suppressing it to keep the output clean and focused on relevant information.\n",
    "    warnings.filterwarnings(action='ignore', category=FutureWarning, module='ISLP.models.columns', lineno=151)\n",
    "    \n",
    "    rng = np.random.default_rng(seed)\n",
    "    first_, second_ = 0, 0\n",
    "    n = n or D.shape[0]\n",
    "    samples = []\n",
    "    # for _ in range(B):: This loop runs B times (1000), where B is the number of bootstrap iterations. In each iteration, \n",
    "    # a bootstrap sample is drawn and the statistic is calculated on that sample.\n",
    "    for _ in range(B):\n",
    "        # each time it enters, it takes the indexes and makes replacements over each iteration\n",
    "        # idx = rng.choice(D.index, n, replace=True): This line generates a random sample of indices (idx) from the \n",
    "        # dataset D. n is the size of each bootstrap sample, and replace=True indicates that sampling is done with \n",
    "        # replacement, meaning that the same index can be selected multiple times in the sample.\n",
    "        idx = rng.choice(D.index,\n",
    "                         n,\n",
    "                         replace=True)\n",
    "        # value = func(D, idx): This line calculates the statistic (func) on the bootstrap sample specified by the indices idx.\n",
    "        value = func(D, idx)\n",
    "        # first_ += value and second_ += value**2: These lines accumulate the statistic and its square across all bootstrap \n",
    "        # samples. These values will be used later to calculate the variance of the statistic.\n",
    "        first_ += value\n",
    "        second_ += value**2\n",
    "        # samples.append(value): This line appends the calculated statistic for the current bootstrap sample to the samples list.\n",
    "        samples.append(value)\n",
    "\n",
    "        # After the loop, the function returns two values:\n",
    "        # 1. samples: This is a list containing the calculated statistic for each bootstrap sample. It provides a distribution \n",
    "        # of the statistic under bootstrapping.\n",
    "        # 2. np.sqrt(second_ / B - (first_ / B)**2): This calculates the standard deviation of the statistic estimate based on \n",
    "        # the bootstrap samples. It uses the accumulated values first_ and second_ to calculate the variance of the statistic estimate.\n",
    "    return samples, np.sqrt(second_ / B - (first_ / B)**2) # formula for standard error; sum of all the values divided by B, squared"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc49225f-368c-467a-90c8-be415fc10a46",
   "metadata": {},
   "source": [
    "Letâ€™s use our function to evaluate the accuracy of our estimate of $\\alpha$ using $B = 1,000$ bootstrap replications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c693db4-e496-4c57-99db-f558c6213874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09118176521277607"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples, alpha_SE = boot_SE(alpha_func,\n",
    "                   Portfolio,\n",
    "                   B=1000,\n",
    "                   seed=0)\n",
    "alpha_SE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb795726",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5821442531025649"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73dc2b7",
   "metadata": {},
   "source": [
    "We got a value of 0.582. that is half way between both our first predictions of 0.575 and 0.607. With a standard error of 0.09.\n",
    "\n",
    "This is a way better estimate of our alpha parameter. \n",
    "\n",
    "If you just run samples, if will give you the results for each sample.\n",
    "\n",
    "Law of Large Numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ee815d-2369-45bb-862f-baddb290820d",
   "metadata": {},
   "source": [
    "The final output shows that the bootstrap estimate for ${\\rm SE}(\\hat{\\alpha})$ is $0.0912$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b759abb-913b-40bf-b11e-890681f1f9e4",
   "metadata": {},
   "source": [
    "The bootstrap approach can be used to assess the variability of the coefficient estimates and predictions from a statistical learning method. Here we use the bootstrap approach in order to assess the variability of the estimates for \n",
    " and \n",
    ", the intercept and slope terms for the linear regression model that uses horsepower to predict mpg in the Auto data set\n",
    "\n",
    "We start by writing a generic function `boot_OLS()` for bootstrapping a regression model that takes a formula to define the corresponding regression. We use the `clone()` function to make a copy of the formula that can be refit to the new dataframe. This means that any derived features such as those defined by `poly()` which will be re-fit on the resampled data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "14434d6e-89ec-4133-8898-85d4ad811dfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[39.88064456, -0.1567849 ],\n",
       "       [38.73298691, -0.14699495],\n",
       "       [38.31734657, -0.14442683],\n",
       "       [39.91446826, -0.15782234],\n",
       "       [39.43349349, -0.15072702],\n",
       "       [40.36629857, -0.15912217],\n",
       "       [39.62334517, -0.15449117],\n",
       "       [39.0580588 , -0.14952908],\n",
       "       [38.66688437, -0.14521037],\n",
       "       [39.64280792, -0.15555698]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def boot_OLS(model_matrix, response, D, idx):\n",
    "    D_ = D.loc[idx]\n",
    "    Y_ = D_[response]\n",
    "    X_ = clone(model_matrix).fit_transform(D_)\n",
    "    return sm.OLS(Y_, X_).fit().params\n",
    "\n",
    "hp_func = partial(boot_OLS, MS(['horsepower']), 'mpg')\n",
    "\n",
    "# Use the hp_func() function to create bootstrap estimates for the intercept and slope \n",
    "rng = np.random.default_rng(0)\n",
    "np.array([hp_func(Auto,\n",
    "          rng.choice(392,\n",
    "                     392,\n",
    "                     replace=True)) for _ in range(10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94384a9f-5e10-4854-9f89-1cce12b51731",
   "metadata": {},
   "source": [
    "Next, we use the `boot_SE()` {} function to compute the standard errors of 1,000 bootstrap estimates for the intercept and slope terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "483dee5f-e600-482f-b41f-b324570225d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "intercept     0.848807\n",
       "horsepower    0.007352\n",
       "dtype: float64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp_se = boot_SE(hp_func,\n",
    "                Auto,\n",
    "                B=1000,\n",
    "                seed=10)\n",
    "hp_se"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5748d1ef-eb30-4538-85b3-78c619eb4d71",
   "metadata": {},
   "source": [
    "The bootstrap estimate for ${\\rm SE}(\\hat{\\beta}_0)$ is\n",
    "0.85, and that the bootstrap\n",
    "estimate for ${\\rm SE}(\\hat{\\beta}_1)$ is\n",
    "0.0074. Standard formulas can be used to compute the standard errors for the regression coefficients in a linear model. These can be obtained using the `summarize()` function from `ISLP.sm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f46fca2a-d45e-47d8-ac66-5aa0b110f16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Dan/miniconda3/envs/learners/lib/python3.11/site-packages/ISLP/models/__init__.py:49: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  results_table = pd.read_html(tab.as_html(),\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "intercept     0.717\n",
       "horsepower    0.006\n",
       "Name: std err, dtype: float64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp_model.fit(Auto, Auto['mpg'])\n",
    "model_se = summarize(hp_model.results_)['std err']\n",
    "model_se"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80895997-b92f-46e0-a329-d9bff98d8957",
   "metadata": {},
   "source": [
    "The standard error estimates for $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ are 0.717 for the intercept and 0.006 for the slope.\n",
    "\n",
    "Now, we want to compute the boostrap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "efde4625-8e96-404b-a4e2-89ecf2f6faab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "intercept                                  2.067840\n",
       "poly(horsepower, degree=2, raw=True)[0]    0.033019\n",
       "poly(horsepower, degree=2, raw=True)[1]    0.000120\n",
       "dtype: float64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quad_model = MS([poly('horsepower', 2, raw=True)])\n",
    "quad_func = partial(boot_OLS,\n",
    "                    quad_model,\n",
    "                    'mpg')\n",
    "boot_SE(quad_func, Auto, B=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9be038-2582-4e02-81f7-e420b579127c",
   "metadata": {},
   "source": [
    "We compare the results to the standard errors computed using `sm.OLS()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a94305c4-bfe3-4d56-b7d3-7c00a3d9b3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Dan/miniconda3/envs/learners/lib/python3.11/site-packages/ISLP/models/__init__.py:49: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  results_table = pd.read_html(tab.as_html(),\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "intercept                                  1.800\n",
       "poly(horsepower, degree=2, raw=True)[0]    0.031\n",
       "poly(horsepower, degree=2, raw=True)[1]    0.000\n",
       "Name: std err, dtype: float64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M = sm.OLS(Auto['mpg'],\n",
    "           quad_model.fit_transform(Auto))\n",
    "summarize(M.fit())['std err']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278c6959-9713-480a-bbe7-b58502a9d49f",
   "metadata": {},
   "source": [
    "*These exercises were adapted from :* James, Gareth, et al. An Introduction to Statistical Learning: with Applications in Python, Springer, 2023."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

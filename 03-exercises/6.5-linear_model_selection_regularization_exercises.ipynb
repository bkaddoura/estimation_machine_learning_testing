{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "382163a6-06e6-4fe0-a88d-b417e3d4d7ec",
   "metadata": {},
   "source": [
    "# 6.5: Linear Model Selection and Regularization Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841e69a3-bc92-4956-a89a-3b774dea134f",
   "metadata": {},
   "source": [
    "### Getting Started\n",
    "\n",
    "#### Import Libraries \n",
    "\n",
    "We import our standard libraries and specific objects/libraries at the top level of our notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3335f7c9-c115-405b-9413-8bbafb155d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and objects\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from matplotlib.pyplot import subplots\n",
    "from statsmodels.api import OLS\n",
    "import sklearn.model_selection as skm\n",
    "import sklearn.linear_model as skl\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from ISLP import load_data\n",
    "from ISLP.models import (ModelSpec as MS,\n",
    "                         summarize)\n",
    "from sklearn.linear_model import ElasticNetCV, ElasticNet\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from ISLP.models import \\\n",
    "     (Stepwise,\n",
    "      sklearn_selected,\n",
    "      sklearn_selection_path)\n",
    "# !pip install l0bnb > nul 2>&1 # \"> nul 2>&1\" means that the install messages have been surpressed\n",
    "from l0bnb import fit_path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Suppress FutureWarning in ISLP.models.columns\n",
    "# The warning is related to Series.__getitem__ treating keys as positions, which is deprecated.\n",
    "# Since ISLP is an external library that I don't control, and this specific warning does not\n",
    "# affect my current usage, I'm suppressing it to keep the output clean and focused on relevant information.\n",
    "warnings.filterwarnings(action='ignore', category=FutureWarning, module='ISLP.models.columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2d013f-23d9-4bd8-a6fc-0944084bc6c6",
   "metadata": {},
   "source": [
    "We will be using the `Hitters` data set in the `ISLP` package. It contains information about a baseball playerâ€™s `Salary`\n",
    "along with other information about their performance in the previous year.\n",
    "\n",
    "The `Salary` variable is missing for some of the players. The `np.isnan()` function can be used to identify the missing observations. It returns an array of the same shape as the input vector, with a `True` for any elements that are missing, and a `False` for non-missing elements. The `sum()` method can then be used to count all of the missing elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea413e8-14c8-4146-9e02-f8957359d4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "Hitters = load_data('Hitters')\n",
    "np.isnan(Hitters['Salary']).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49244fb6-6f0c-47f5-860c-16c4f1586f5a",
   "metadata": {},
   "source": [
    "The `dropna()` method of data frames removes all of the rows that have missing values in any variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209d3600-5c95-4e76-9380-0859e58aa05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Hitters = Hitters.dropna();\n",
    "Hitters.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf7f10e-36a3-4395-8594-0b288e4051ab",
   "metadata": {},
   "source": [
    "### Subset Selection & Deciding Between Models Using the Validation Set Approach and Cross-Validation\n",
    "#### Best Subset Selection & Stepwise Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9eb8ae-f7a5-4e61-abc1-2195adbd3346",
   "metadata": {},
   "source": [
    "We first choose the best model using forward selection based on $C_p$. This score\n",
    "is not built in as a metric to `sklearn`, so we have to define the function ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11443a2a-5a8a-4f34-9a0d-2192e40c4a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nCp(sigma2, estimator, X, Y):\n",
    "    \"Negative Cp statistic\"\n",
    "    n, p = X.shape\n",
    "    Yhat = estimator.predict(X)\n",
    "    RSS = np.sum((Y - Yhat)**2)\n",
    "    return -(RSS + 2 * p * sigma2) / n \n",
    "\n",
    "design = MS(Hitters.columns.drop('Salary')).fit(Hitters)\n",
    "Y = np.array(Hitters['Salary'])\n",
    "X = design.transform(Hitters)\n",
    "sigma2 = OLS(Y,X).fit().scale\n",
    "\n",
    "neg_Cp = partial(nCp, sigma2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b107b0ca-8859-43aa-b7f0-e634cfff2489",
   "metadata": {},
   "source": [
    "We can now use `neg_Cp()` as a scorer for model selection. Along with a score we need to specify the search strategy. This is done through the object `Stepwise()` in the `ISLP.models` package, using the method `Stepwise.first_peak()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8be8f3-f2f1-40b3-93c5-79f1c3006c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For fixed number of steps of stepwise search\n",
    "strategy = Stepwise.first_peak(design,\n",
    "                               direction='forward',\n",
    "                               max_terms=len(design.terms))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7947030-3bcc-4ff2-9802-42c04933075f",
   "metadata": {},
   "source": [
    "We now fit a linear regression model with `Salary` as outcome using forward\n",
    "selection. To do so, we use the function `sklearn_selected()`  from the `ISLP.models` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf467ac-63a3-4e15-9289-df128339cd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hitters_MSE = sklearn_selected(OLS,\n",
    "                               strategy)\n",
    "hitters_MSE.fit(Hitters, Y)\n",
    "hitters_MSE.selected_state_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb063e80-0cb7-4fd1-9e71-632c9ff54311",
   "metadata": {},
   "outputs": [],
   "source": [
    "hitters_Cp = sklearn_selected(OLS,\n",
    "                               strategy,\n",
    "                               scoring=neg_Cp)\n",
    "hitters_Cp.fit(Hitters, Y)\n",
    "hitters_Cp.selected_state_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dded8c6-9cac-4a24-9e12-92b7fca1503b",
   "metadata": {},
   "source": [
    "As an  alternative to using $C_p$, we might try cross-validation to select a model in forward selection. For this, we need a\n",
    "method that stores the full path of models found in forward selection, and allows predictions for each of these. This can be done with the `sklearn_selection_path()` estimator from `ISLP.models`. The function `cross_val_predict()` from `ISLP.models`\n",
    "computes the cross-validated predictions for each of the models along the path, which we can use to evaluate the cross-validated MSE along the path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df539a5-9f3d-4855-8543-09471a3bde57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path\n",
    "strategy = Stepwise.fixed_steps(design,\n",
    "                                len(design.terms),\n",
    "                                direction='forward')\n",
    "full_path = sklearn_selection_path(OLS, strategy)\n",
    "\n",
    "# Fit the full forward-selection path\n",
    "full_path.fit(Hitters, Y)\n",
    "Yhat_in = full_path.predict(Hitters)\n",
    "Yhat_in.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1195c69-ebd1-466f-ade7-a2054b853cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the cross-validation and validation set MSE\n",
    "mse_fig, ax = subplots(figsize=(8,8))\n",
    "insample_mse = ((Yhat_in - Y[:,None])**2).mean(0)\n",
    "n_steps = insample_mse.shape[0]\n",
    "ax.plot(np.arange(n_steps),\n",
    "        insample_mse,\n",
    "        'k', # color black\n",
    "        label='In-sample')\n",
    "ax.set_ylabel('MSE',\n",
    "              fontsize=20)\n",
    "ax.set_xlabel('# steps of forward stepwise',\n",
    "              fontsize=20)\n",
    "ax.set_xticks(np.arange(n_steps)[::2])\n",
    "ax.legend()\n",
    "ax.set_ylim([50000,250000]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8235af-58b5-4082-8ea9-6b6cb8fe2484",
   "metadata": {},
   "source": [
    "We are now ready to use cross-validation to estimate test error along the model path. We must use only the training observations to perform all aspects of model-fitting - including variable selection.\n",
    "\n",
    "We now compute the cross-validated predicted values using 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb27c2d-10de-4fcf-9c3a-826627d821f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 5\n",
    "kfold = skm.KFold(K,\n",
    "                  random_state=0,\n",
    "                  shuffle=True)\n",
    "Yhat_cv = skm.cross_val_predict(full_path,\n",
    "                                Hitters,\n",
    "                                Y,\n",
    "                                cv=kfold)\n",
    "Yhat_cv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e285a60-7e7d-4f27-a869-d41cff4143c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_mse = []\n",
    "for train_idx, test_idx in kfold.split(Y):\n",
    "    errors = (Yhat_cv[test_idx] - Y[test_idx,None])**2\n",
    "    cv_mse.append(errors.mean(0)) # column means\n",
    "cv_mse = np.array(cv_mse).T\n",
    "cv_mse.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063666d4-2e61-4dcd-80d4-d1178b91f27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add cross validation error estimates \n",
    "ax.errorbar(np.arange(n_steps), \n",
    "            cv_mse.mean(1),\n",
    "            cv_mse.std(1) / np.sqrt(K),\n",
    "            label='Cross-validated',\n",
    "            c='r') # color red\n",
    "ax.set_ylim([50000,250000])\n",
    "ax.legend()\n",
    "mse_fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe7496d-e22b-43a4-a715-3bdff732829e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat using the validation set approach\n",
    "validation = skm.ShuffleSplit(n_splits=1, \n",
    "                              test_size=0.2,\n",
    "                              random_state=0)\n",
    "for train_idx, test_idx in validation.split(Y):\n",
    "    full_path.fit(Hitters.iloc[train_idx],\n",
    "                  Y[train_idx])\n",
    "    Yhat_val = full_path.predict(Hitters.iloc[test_idx])\n",
    "    errors = (Yhat_val - Y[test_idx,None])**2\n",
    "    validation_mse = errors.mean(0)\n",
    "\n",
    "# Plot\n",
    "ax.plot(np.arange(n_steps), \n",
    "        validation_mse,\n",
    "        'b--', # color blue, broken line\n",
    "        label='Validation')\n",
    "ax.set_xticks(np.arange(n_steps)[::2])\n",
    "ax.set_ylim([50000,250000])\n",
    "ax.legend()\n",
    "mse_fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac5e837-de4b-47fb-afc7-626138df9dcb",
   "metadata": {},
   "source": [
    "#### Best Subset Selection\n",
    "We will use a package called `l0bnb` to perform best subset selection on the `Hitters` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62fe5f9-ea4a-479c-9477-389b842d02e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = design.fit_transform(Hitters)\n",
    "D = D.drop('intercept', axis=1)\n",
    "X = np.asarray(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e6fafd-a842-4632-880f-705a4419e152",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = fit_path(X,\n",
    "Y,\n",
    "max_nonzeros=X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5707b50a-97e9-41f4-be8c-52c160e0fd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "path[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0d4a0d-1844-4eb7-9673-632c375c4575",
   "metadata": {},
   "source": [
    "We could make predictions using this sequence of fits on a validation set as a function of `lambda_0`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3022e78-b3d5-4f65-964f-fb26fc1d1462",
   "metadata": {},
   "source": [
    "### Ridge Regression & The Lasso\n",
    "#### Ridge Regression\n",
    "We will use the `sklearn.linear_model` package (for which\n",
    "we use `skl` as shorthand below)\n",
    "to fit ridge and  lasso regularized linear models on the `Hitters` data.\n",
    "We start with the model matrix `X` (without an intercept).\n",
    "\n",
    "We will use the function `skl.ElasticNet()` to fit both  ridge and the lasso.\n",
    "To fit a *path* of ridge regressions models, we use\n",
    "`skl.ElasticNet.path()`, which can fit both ridge and lasso, as well as a hybrid mixture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4049cebf-7f6f-42fb-8982-c47ac325b60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs = X - X.mean(0)[None,:]\n",
    "X_scale = X.std(0)\n",
    "Xs = Xs / X_scale[None,:]\n",
    "lambdas = 10**np.linspace(8, -2, 100) / Y.std()\n",
    "soln_array = skl.ElasticNet.path(Xs,\n",
    "                                 Y,\n",
    "                                 l1_ratio=0.,\n",
    "                                 alphas=lambdas)[1]\n",
    "soln_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe07b8dd-b460-4946-9fa2-cda664544132",
   "metadata": {},
   "outputs": [],
   "source": [
    "soln_path = pd.DataFrame(soln_array.T,\n",
    "                         columns=D.columns,\n",
    "                         index=-np.log(lambdas))\n",
    "soln_path.index.name = 'negative log(lambda)'\n",
    "soln_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bedd83-2e0a-4a11-a633-5f874b2fd801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "path_fig, ax = subplots(figsize=(8,8))\n",
    "soln_path.plot(ax=ax, legend=False)\n",
    "ax.set_xlabel('$-\\log(\\lambda)$', fontsize=20)\n",
    "ax.set_ylabel('Standardized coefficients', fontsize=20)\n",
    "ax.legend(loc='upper left');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875cdff8-13c8-4403-9ebc-6bb80b300b43",
   "metadata": {},
   "source": [
    "We expect the coefficient estimates to be much smaller, in terms of\n",
    "$\\ell_2$ norm, when a large value of $\\lambda$ is used, as compared to\n",
    "when a small value of $\\lambda$ is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15788cad-979c-4413-9b1a-6e4e35673da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_hat = soln_path.loc[soln_path.index[39]]\n",
    "lambdas[39], beta_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d7ca3e-60c2-47b1-82f8-52fe00af6559",
   "metadata": {},
   "source": [
    "Letâ€™s compute the $\\ell_2$ norm of the standardized coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5db0d3-46b9-49bf-9757-935184f21de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(beta_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b74908-5680-4693-95c2-f9cc1b46509d",
   "metadata": {},
   "source": [
    "In contrast, here is the $\\ell_2$ norm when $\\lambda$ is 2.44e-01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9173a51-c778-4e54-9ee1-42893f6dfac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_hat = soln_path.loc[soln_path.index[59]]\n",
    "lambdas[59], np.linalg.norm(beta_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2957dac9-0c58-44f1-b86a-51f6f1590964",
   "metadata": {},
   "source": [
    "Above we normalized `X` upfront, and fit the ridge model using `Xs`.\n",
    "The `Pipeline()`  object\n",
    "in `sklearn` provides a clear way to separate feature\n",
    "normalization from the fitting of the ridge model itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70e359a-6591-49f4-851d-74711a8c7e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = skl.ElasticNet(alpha=lambdas[59], l1_ratio=0)\n",
    "scaler = StandardScaler(with_mean=True,  with_std=True)\n",
    "pipe = Pipeline(steps=[('scaler', scaler), ('ridge', ridge)])\n",
    "pipe.fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219a3f45-9f38-4c03-8efd-5f9dfd228990",
   "metadata": {},
   "source": [
    "We show that it gives the same $\\ell_2$ norm as in our previous fit on the standardized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce446583-987d-4b46-810a-c774d05dc59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(ridge.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8b9abc-2d05-4953-bf0d-9985e5b99155",
   "metadata": {},
   "source": [
    "#### The Lasso\n",
    "We now ask whether the lasso can yield\n",
    "either a more accurate or a more interpretable model than ridge\n",
    "regression. In order to fit a lasso model, we once again use the\n",
    "`ElasticNetCV()`  function; however, this time we use the argument\n",
    "`l1_ratio=1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c18dbb-c54c-4759-8db6-135c8738d5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lassoCV = skl.ElasticNetCV(n_alphas=100, \n",
    "                           l1_ratio=1,\n",
    "                           cv=kfold)\n",
    "pipeCV = Pipeline(steps=[('scaler', scaler),\n",
    "                         ('lasso', lassoCV)])\n",
    "pipeCV.fit(X, Y)\n",
    "tuned_lasso = pipeCV.named_steps['lasso']\n",
    "tuned_lasso.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9010e056-74d5-4731-8603-6417f60f08da",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas, soln_array = skl.Lasso.path(Xs, \n",
    "                                    Y,\n",
    "                                    l1_ratio=1,\n",
    "                                    n_alphas=100)[:2]\n",
    "soln_path = pd.DataFrame(soln_array.T,\n",
    "                         columns=D.columns,\n",
    "                         index=-np.log(lambdas))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e32fba-db71-4345-94d5-7564234165c4",
   "metadata": {},
   "source": [
    "We can see from the coefficient plot of the standardized coefficients that depending on the choice of\n",
    "tuning parameter, some of the coefficients will be exactly equal to\n",
    "zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8af26f-a3e9-4e7f-a5b8-3b163c3c6000",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_fig, ax = subplots(figsize=(8,8))\n",
    "soln_path.plot(ax=ax, legend=False)\n",
    "ax.legend(loc='upper left')\n",
    "ax.set_xlabel('$-\\log(\\lambda)$', fontsize=20)\n",
    "ax.set_ylabel('Standardized coefficiients', fontsize=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0493a0-1b5d-4bb0-b8ef-06ea9e6a0493",
   "metadata": {},
   "source": [
    "The smallest cross-validated error is lower than the test set MSE of the null model\n",
    "and of least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65dcf123-8b69-4a78-869d-a531144f4e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(tuned_lasso.mse_path_.mean(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ab1976-e60b-4083-9aae-469bcbeebfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cross-validation error\n",
    "lassoCV_fig, ax = subplots(figsize=(8,8))\n",
    "ax.errorbar(-np.log(tuned_lasso.alphas_),\n",
    "            tuned_lasso.mse_path_.mean(1),\n",
    "            yerr=tuned_lasso.mse_path_.std(1) / np.sqrt(K))\n",
    "ax.axvline(-np.log(tuned_lasso.alpha_), c='k', ls='--')\n",
    "ax.set_ylim([50000,250000])\n",
    "ax.set_xlabel('$-\\log(\\lambda)$', fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72943117-7524-4ac8-8a8c-dd3cabfa624b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_lasso.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67ed00c-04a9-4775-945a-df48bfa723ea",
   "metadata": {},
   "source": [
    "The lasso has a substantial advantage over ridge regression\n",
    "in that the resulting coefficient estimates are sparse. Here we see\n",
    "that 6 of the 19 coefficient estimates are exactly zero. So the lasso\n",
    "model with $\\lambda$ chosen by cross-validation contains only 13\n",
    "variables.\n",
    "\n",
    "As in ridge regression, we could evaluate the test error\n",
    "of cross-validated lasso by first splitting into\n",
    "test and training sets and internally running\n",
    "cross-validation on the training set. We leave\n",
    "this as an exercise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c59d4ce-a7a3-4d1a-a168-9bb1dcc36e5b",
   "metadata": {},
   "source": [
    "**Fit a linear regression model to the data. Compute the test MSE for the linear regression\n",
    "model. Compare the results from the linear regression, ridge regression, and the lasso. Which\n",
    "is the best model and why?**\n",
    "\n",
    "*These exercises were adapted from : James, Gareth, et al. An Introduction to Statistical Learning: with Applications in Python, Springer, 2023.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

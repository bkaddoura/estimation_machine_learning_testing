---
marp: true
theme: dsi-certificates-theme
_class: invert
paginate: true
math: mathjax
---

# 6.3: Classification

```code
$ echo "Data Science Institute"
```

---

# Intro

Classification involves predicating a qualitative response by a assigning it to a category. The methods that are used to classify observations are called **classifiers** and most of them work by following two steps:

-   Compute the probability that an observation belongs to a category.

-   Classify the observation based on some probability threshold (i.e. if the probability that an observation belongs to some category is greater than 0.5 then assign the observation to that category)

---
probability that observation belongs to a category.



# Why not use linear regression?

Suppose we are trying to diagnose a patient with either a *stroke*, *drug overdose*, or *epileptic seizure* based on their symptoms. We can code this response as follows 
> $
    Y=\left\{\begin{array}{ll}
1 & \text { if stroke; } \\
2 & \text { if drug overdose; } \\
3 & \text { if epileptic seizure. }
\end{array}\right.
$ At this point we could use linear regression to predict $Y$ based on a set of predictors. However there are several problems with this coding

-   Implies an ordering of the outcomes.

-   The difference between epileptic seizure and stroke versus stroke and drug overdose is assumed to be the same.

---
Category that has multiple variables. Problems are: Linear regression implies order; Assuming difference between variables remain constant.

# Why not use linear regression?

Suppose we are trying to diagnose a patient with either a *stroke*, *drug overdose*, or *epileptic seizure* based on their symptoms. We can code this response as follows 
> $
    Y=\left\{\begin{array}{ll}
1 & \text { if stroke; } \\
2 & \text { if drug overdose; } \\
3 & \text { if epileptic seizure. }
\end{array}\right.
$ At this point we could use linear regression to predict $Y$ based on a set of predictors. However there are several problems with this coding

A different ordering would give completely different results for the linear regression. _**♦️There is no convenient way to code a qualitative response with more than two levels so that linear regression can be used.♦️**_

---



# Why not use linear regression?

The 0/1 coding for a binary qualitative response variable does not suffer the same problems. However the probabilities we obtain will be difficult to interpret

-   negative probabilities

-   probabilities above 1

So, linear regression only able to give _**♦️crude estimates of the probabilities for a binary response.♦️**_

In summary, we don't use linear regression for classification since:

-   It does not work for a qualitative response variable with more than 2 classes.

-   With 2 classes, the probability estimates are not meaningful.

---
IMPORTANT
If we want to code as a binary input. Problems: negative probabilites; probabilities above 1.
If we have 2 classes we can use linear regression; If more than 2 classes we cant.
Capture summaries.

# Logistic Regression

**Logistic regression** models the probability that the response $Y$ belongs to a particular category. Suppose we have a qualitative response $Y$ that has two levels, coded as 0 and 1, and one predictor variable. We want to model


> $
p(X)=\operatorname{Pr}(Y=1 \mid X)
$

The logistic function keeps the probabilities between 0 and 1. For one predictor, the function is 
> $
p(X)=\frac{e^{\beta_{0}+\beta_{1} X}}{1+e^{\beta_{0}+\beta_{1} X}}
$ As with linear regression, we are trying to fit $\beta_0, \beta_1$.

---
Given some X, what is the probability of Y? What is the Y with the probabbility of 1 given our X is present. Allows us to keep our values between 0 and 1. Linear regression has it's limitations. 

# Estimating the regression coefficients

$\beta_0$ and $\beta_1$ are estimated using the training data using a method called **maximum likelihood**. This involves maximizing the likelihood function, but we will not cover the details of this function.

---
How do I maximize the likelihood that a classified smoker is likely to die of a heartattack?

# Odds

The **odds** compares the probability of a particular outcome to the probability of all the other outcomes.


> $
    \frac{p(X)}{1-p(X)}=e^{\beta_{0}+\beta_{1} X}
$

-   takes values between (0, $\infty$)

-   odds close to 0 $\Rightarrow$ very low probability of the outcome in question

-   odds much greater than 0 $\Rightarrow$ very high probability of the outcome in question.

---
Flip in the probability space is opposite. If we know that the probability of a heart attack is 30% then we know that 70% won't have a heart attack.

# Log Odds

The **log odds** (or logit) is obtained by taking the logarithm of the odds 
> $
\log \left(\frac{p(X)}{1-p(X)}\right)=\beta_{0}+\beta_{1} X
$

-   Increasing $X$ by one unit changes the log odds by $\beta_1$.

-   If $\beta_1$ is positive, increasing $X$ is associated with increasing $p(X)$

-   If $\beta_1$ is negative, increasing $X$ is associated with decreasing $p(X)$

---
Will see often. Taking log value of the odds. log value will always be between 0 and 1. More interprettable value. Changes in X is equal to increase in Beta 1.Impact on Y is related to increase in Beta 1. As exercise (X1) increases, odds of having a heart attack (Y) decreases with Beta1 as negative.
P(Heart Attack | Exercise = 1)

# Making Predictions

Once the coefficients have been estimated predictions can be made for any value of the predictor. Logistic regression will give the probability of the outcome and the classification will be according to some threshold which depends on the problem or how conservative the predictions should be.

---
High-Low-No Risk depends set threshold. Make it broad to include as many as possible. Can put predictions accordingly. 

# Multiple Predictors

Simple logistic regression can be extended to include multiple predictors 
> $
p(X)=\frac{e^{\beta_{0}+\beta_{1} X_{1}+\cdots+\beta_{p} X_{p}}}{1+e^{\beta_{0}+\beta_{1} X_{1}+\cdots+\beta_{p} X_{p}}}$

The log odds in this case becomes 
> $
\log \left(\frac{p(X)}{1-p(X)}\right)=\beta_{0}+\beta_{1} X_{1}+\cdots+\beta_{p} X_{p}$

As before, the maximum likelihood is used to estimate the coefficients.

---
Can include multiple predictors and still get value of Y. Still get log odds to take form of this equation. We can expand to include more regressors.

# Exercise: Logistic Regression

Open the Classification Exercises R Markdown or Jupyter Notebook file.

-   Go over "Getting Started" together as a class.

-   Go through the "Logistic Regression" as a class.

-   5 minutes for students to complete the questions from "Logistic Regression".

-   Questions should be completed at home if time does not allow.

---

# Multinomial logistic regression

We can extend to two-class logistic regression to accommodate $K$ classes. We need to select one class to serve as the **baseline**, so we will choose the $K$th class. Then the model becomes 

> $\operatorname{Pr}(Y=K \mid X=x)=\frac{1}{1+\sum_{l=1}^{K-1} e^{\beta_{l 0}+\beta_{l 1} x_{1}+\cdots+\beta_{l p} x_{p}}}$

and,

> $\operatorname{Pr}(Y=k \mid X=x)=\frac{e^{\beta_{k 0}+\beta_{k 1} x_{1}+\cdots+\beta_{k p} x_{p}}}{1+\sum_{l=1}^{K-1} e^{\beta_{l 0}+\beta_{l 1} x_{1}+\cdots+\beta_{l p} x_{p}}} \text{for } k = 1, \dots, K-1$

The interpretation of the coefficients is tied to the choice of the baseline.

---
As many classes as possible; 
K = 1 because we are removing the value of our baseline.
Measuring against small (k).


# Bayes Classifier

Suppose that we have a qualitative response variable $Y$ with $K$ distinct and ordered classes. The Bayes classifier use a less direct approach using Bayes' theorem to estimating the probabilities

> $\operatorname{Pr}(Y=k \mid X=x)=\frac{\pi_{k} f_{k}(x)}{\sum_{l=1}^{K} \pi_{l} f_{l}(x)}$

-   $\pi_k$ is the **prior** probability that a random observation belongs to the $k$th class.

    -   estimated as the fraction of the training observation that belong to the $k$th class.

-   $f_{k}(X) \equiv \operatorname{Pr}(X \mid Y=k)$ is the **density function** of $X$ for an observation from the $k$th class.

There are several methods we will discuss that attempt to approximate the Bayes classifier using different approaches for estimating $f_k(x)$.

---
All about estimating probabilities. Does it fall into this category and why?
(K) is how many classes. Umbrella term. (k) is for each class.(1, 2,...)
Using to predict prior probibility. Populating mean and ...

# Why Use Bayes Classifier?

-   When there is _**♦️a lot of separation between two classes♦️**_ logistic regression does not does not provide stable coefficient estimates.

-   If the _**♦️distribution of each of the predictors is approximately normal and the sample size is small♦️**_, these approaches are more accurate.

The methods that attempt to estimate the Bayes classifier that we will cover are:

-   Linear discriminant analysis,

-   Quadratic discriminant analysis, and

-   Naive Bayes.

---
Alot of seperation between classes. Logistic regration can't account for this. Logistic regression can't capture seperation pieces. In a Bell Chart, the Bayes classifier is more accurate than logistic regression.
Second point is the bigger assumption. 

# Linear Discriminant Analysis

Suppose we only have one predictor, so $p = 1$. In order to estimate $f_k(x)$ we make the following assumptions:

-   $f_k(x)$ is normal

-   the variance is the same across all $K$ classes.

Linear discriminant analysis (LDA) then approximates the Bayes classifier using the estimates: 
> $\hat{\mu}_{k} =\frac{1}{n_{k}} \sum_{i: y_{i}=k} x_{i} \quad (k \text{th mean)}$

> $\hat{\sigma}^{2} =\frac{1}{n-K} \sum_{k=1}^{K} \sum_{i: y_{i}=k}\left(x_{i}-\hat{\mu}_{k}\right)^{2} \quad \text{(variance)}$

> $\hat{\pi}_{k}=n_{k} / n \quad (k \text{th prior probability)}$

where $n$ = number of training observations, and $n_k$ = number of observations in the $k$th class.

---
Supposes that we only have one predictor. 

# Linear Discriminant Analysis

The LDA classifier uses the estimates for $\pi_{k}, \mu_{k}$, and $\sigma^{2}$ to assign an observation $X = x$ to the class that has the largest 
> $
\hat{\delta}_{k}(x)=x \cdot \frac{\hat{\mu}_{k}}{\hat{\sigma}^{2}}-\frac{\hat{\mu}_{k}^{2}}{2 \hat{\sigma}^{2}}+\log \left(\hat{\pi}_{k}\right)
> $

---
Using LDA to get Bayes classifier.What is the averae how close are we to the average and ...

# Linear Discriminant Analysis for $p>1$

Now suppose we have $X = (X_1, \dots, X_p)$ predictors. Assumptions: $X$ is multivariate Gaussian (i.e. each predictor is normally distributed with some correlation between them)

-   class-specific mean vectors

-   common covariance matrix across classes.

We classify observations to the class for which $\hat{\delta}_{k}(x)$ is the largest.

---
Start with single parameter and singler predictor. Covariance: when one moves, how does the other? Relationship.
Use LDA where probabilites will be the highest.


# Binary Classifiers

Binary classifiers, similarly to such as tests for diseases (positive versus negative), can make two types of errors:

-   Incorrectly assign an individual as positive when they are negative (False positive).

-   Incorrectly assign an individual as negative when they are positive (False negative).

---
Cases where we have classifiers where we have a yes or no. Binary.
This is what we would use when we have two outcomes, or classes. Helps us create a confusion matrix.

# Confusion Matrix

A confusion matrix helps to summarize the two types of errors of binary classifiers. They compare the LDA predictions to the true outcomes of the training observations. In the case of medical tests this looks like:

![](images/confusionmatrix.png)

The red text is where the numbers are filled in.

---
Where is the highest one and are we ok with that.
How often we get outputs. Want to maximize diagonal of negative negative and positive positive.

# Confusion Matrix

![](images/confusionmatrix.png)

-   N and P are the number of actual negatives and positives respectively in the training data.

-   N\* and P\* are the number of predicted negative and positives in the training data.

---
False Positive is safer than a False Negative.

# Threshold

Recall that the Bayes classifier assigns observations to the class for with the posterior probability is the greatest. Since probabilities sum to 1, for a binary classifier this means that a test will come back positive if:


> $\operatorname{Pr}( \text{positive} \mid X=x) > 0.5$

That is, the binary classifier uses a threshold of 50%. Depending on the classification problem, one may want to specify a different threshold level. For example:


> $\text{positive if }\operatorname{Pr}( \text{positive} \mid X=x) > 0.2$

---
Incremental changes. All probabilities have to sum to 1. Depending on how strict or how conservative we want to be with our threshold level.

# ROC

The ROC (receiver operator characteristics) curve is a method for visualising the errors previously discusses for all possible thresholds.

![](images/ROC.png)

---
Method for visualizing errors.
Make curve as close to corner as possible, closest to False positive being zero.
Determines true positives and false positives. 

# ROC
-   Performance of a classifier is given by the area under the ROC curve, called the AUC.

-   The larger the AUC the better.

-   Ideal ROC curve is as close to the top left corner as possible.

![](images/ROC.png)

---
Maximizing where the Truse Positive is.

# Exercise: Linear Discriminant Analysis

Open the Classification Exercises R Markdown or Jupyter Notebook file.

-   Go over the "Linear Discriminant Analysis" section together as a class.

-   5 minutes for students to complete the questions from "Linear Discriminant Analysis".

-   Questions should be completed at home if time does not allow.

---

# Quadratic Discriminant Analysis

The Quadratic discriminant analysis (QDA) classifier assumes that:

-   observations are drawn from a class-specific Gaussian distribution

-   each class has its own covariance matrix (unlike LDA)

The QDA uses estimates for the class-specific means ($\mu_k$), covariance matrices ($\boldsymbol{\Sigma}_{k}$), and prior probability ($\mu_k$) to assign an observation $x$ to the class for which


> $
\delta_{k}(x)=-\frac{1}{2}\left(x-\mu_{k}\right)^{T} \boldsymbol{\Sigma}_{k}^{-1}\left(x-\mu_{k}\right)-\frac{1}{2} \log \left|\boldsymbol{\Sigma}_{k}\right|+\log \pi_{k}$

is the largest. Unlike the LDA, the function for $\delta_{k}(x)$ is quadratic which gives the QDA it's name.

---
Similar to LDA. Just extending LDA to incorporate more matrices. Every class has its own covariance matrix. Each class has its own matrix. Uses class specific mean. 'K' signifies class. 

# LDA vs QDA

When to use the LDA versus the QDA:

-   LDA is better than QDA when there are few training observations since it requires fewer parameters to be estimated.

-   QDA is best when there are many training observations or when the assumption of a common covariance matrix in the LDA is clearly wrong.

---
When to apply to what context. Link [https://towardsdatascience.com/differences-of-lda-qda-and-gaussian-naive-bayes-classifiers-eaa4d1e999f6]
Link [https://freedium.cfd/https://towardsdatascience.com/svm-and-kernel-svm-fed02bef1200]

# Exercise: Quadratic Discriminant Analysis

Open the Classification Exercises R Markdown or Jupyter Notebook file.

-   Go over the "Quadratic Discriminant Analysis" section together as a class.

-   5 minutes for students to complete the questions from "Quadratic Discriminant Analysis".

-   Questions should be completed at home if time does not allow.

---

# Naive Bayes (quantitative)

The naive Bayes classifier assumes: \textit{within each class, the $p$ predictors are independent}. This allows us to disregard any association between the $p$ predictors and gives the form of $f_k(x)$ as 
> $f_{k}(x)=f_{k 1}\left(x_{1}\right) \times f_{k 2}\left(x_{2}\right) \times \cdots \times f_{k p}\left(x_{p}\right)$

where $f_{k j}$ is the density function of the $j$th predictor for observations in the $k$th class.

To estimate $f_{k j}$ using the training data there are several options:

-   For _**♦️quantitative♦️**_ $X_j$:

    -   assume that for each class, the $j$th predictor is drawn from a normal distribution

    -   or estimate it as the fraction of the training observations in the $k$th class that belong to the same histogram bin as $x_j$.

---



# Naive Bayes (qualitative)

The naive Bayes classifier assumes: \textit{within each class, the $p$ predictors are independent}. This allows us to disregard any association between the $p$ predictors and gives the form of $f_k(x)$ as 
> $
f_{k}(x)=f_{k 1}\left(x_{1}\right) \times f_{k 2}\left(x_{2}\right) \times \cdots \times f_{k p}\left(x_{p}\right)
$ where $f_{k j}$ is the density function of the $j$th predictor for observations in the $k$th class.

To estimate $f_{k j}$ using the training data there are several options:

-   For _**♦️qualitative♦️**_ $X_j$:

    -   count the proportion of training observations for the $j$th predictor that belong to each class.

---

# Exercise: Naive Bayes

Open the Classification Exercises R Markdown or Jupyter Notebook file.

-   Go over the "Naive Bayes" section together as a class.

-   5 minutes for students to complete the questions from "Naive Bayes".

-   Questions should be completed at home if time does not allow.

---

# $K$-Nearest Neighbours

The $K$-nearest neighbors (KNN) classifier works very differently than any of the previous classification methods. For a test observation $x_0$, it identifies $K$ training data points that are closest to $x_0$ (represented by $\mathcal{N}_0$) and estimates the conditional probability for class $j$ as 

> $
\operatorname{Pr}\left(Y=j \mid X=x_{0}\right)=\frac{1}{K} \sum_{i \in \mathcal{N}_{0}} I\left(y_{i}=j\right)
$

where $I(y_i = j)$ if an **indicator variable** that equals 1 is $y_i = j$ and 0 otherwise. The KNN classifier classifies the test observation $x_0$ to the class for which the above probability is the largest.

---
Using distance to say where should a test observation fit. Using things closest to it, conditioning it with a probilitiy and spits out result.
Point one: identify closest to two; estimate conditional 

# $K$-Nearest Neighbours

These figures illustrate the KNN approach with $K=3$. To the left we see the 3 closest points to x are 1 orange and 2 blue so this observation will be classified as blue. The right figure shows the decision boundaries where an observation will be classified as blue or orange.

![](images/KNN1.png)

---
Where can I draw decision boundaries.

# Exercise: K-Nearest Neighbours

Open the Classification Exercises R Markdown or Jupyter Notebook file.

-   Go over the "K-Nearest Neighbours" section together as a class.

-   5 minutes for students to complete the questions from "K-Nearest Neighbours".

-   Questions should be completed at home if time does not allow.

---

# How to choose the classification method

The choice of classification method depends on two things:

-   the true distribution of the predictors in each of the $K$ classes, and

-   the number of training observations ($n$) compared to to the number of predictors ($p$).

---
Depends on distribution of predictors and the number of training observations. 

# References

Chapter 4 and section 2.2.3 of the ISLR2 and ISLP books:

James, Gareth, et al. "Classification." An Introduction to Statistical Learning: with Applications in R, 2nd ed., Springer, 2021.

James, Gareth, et al. "Classification." An Introduction to Statistical Learning: with Applications in Python, Springer, 2023.

Link[https://s3.amazonaws.com/assets.datacamp.com/email/other/ML+Cheat+Sheet_2.pdf]

How to explain ML algorithms to your grandma
Link[https://towardsdatascience.com/machine-learning-algorithms-in-laymans-terms-part-1-d0368d769a7b]

Weapons of Math destruction [https://www.amazon.ca/Weapons-Math-Destruction-Increases-Inequality/dp/0553418815]
